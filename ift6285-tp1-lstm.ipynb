{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ift6285-tp1-lstm.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"ycul1k1wW-_y","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"15f8f412-21cd-4831-d61a-bb7385dc6da2","executionInfo":{"status":"ok","timestamp":1519747013359,"user_tz":300,"elapsed":9615,"user":{"displayName":"Louis Lv","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"101178496584126032730"}}},"cell_type":"code","source":["# https://keras.io/\n","!pip install -q keras\n","import keras"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"H-IVQBM9gRXB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":6}],"base_uri":"https://localhost:8080/","height":138},"outputId":"3dfa8da7-0941-4e98-e5bc-c3e648442df5","executionInfo":{"status":"ok","timestamp":1519747074833,"user_tz":300,"elapsed":12951,"user":{"displayName":"Louis Lv","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"101178496584126032730"}}},"cell_type":"code","source":["# Install a Drive FUSE wrapper.\n","# https://github.com/astrada/google-drive-ocamlfuse\n","!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","\n","# Generate auth tokens for Colab\n","from google.colab import auth\n","auth.authenticate_user()\n","\n","# Generate creds for the Drive FUSE library.\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n","\n","!mkdir -p drive\n","!google-drive-ocamlfuse drive\n","\n","print('Files in Drive:')\n","!ls /content/drive/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n","Files in Drive:\n","Colab Notebooks  created.txt  data  Louis共享文件夹  M.Sc-DIRO-UdeM  test.ods\n"],"name":"stdout"}]},{"metadata":{"id":"0ezTCTnhWOlS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"dff2cfda-bd14-4b29-d121-03ca4235d1ed","executionInfo":{"status":"ok","timestamp":1519748802321,"user_tz":300,"elapsed":843,"user":{"displayName":"Louis Lv","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"101178496584126032730"}}},"cell_type":"code","source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\n","#%%\n","from __future__ import print_function\n","\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","import numpy as np\n","import gzip\n","\n","\n","batch_size = 64  # Batch size for training.\n","epochs = 100  # Number of epochs to train for.\n","latent_dim = 256  # Latent dimensionality of the encoding space.\n","num_samples = 10000  # Number of samples to train on.\n","# Path to the data txt file on disk.\n","\n","import os\n","os.chdir(\"/content/drive/M.Sc-DIRO-UdeM/IFT6285-Traitements automatique des langues naturelles/TP1/ift6285-tp1\")\n","# os.chdir(\"/Users/louis/Google Drive/M.Sc-DIRO-UdeM/IFT6285-Traitements automatique des langues naturelles/TP1\")\n","# os.chdir(\"/Users/fanxiao/Google Drive/UdeM/IFT6135 Representation Learning/homework1/programming part \")\n","print(os.getcwd())\n","\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/M.Sc-DIRO-UdeM/IFT6285-Traitements automatique des langues naturelles/TP1/ift6285-tp1\n"],"name":"stdout"}]},{"metadata":{"id":"C5nsykhhWXcb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":135},"outputId":"72b79e17-7fbb-4ee1-fef4-a222bb16471a","executionInfo":{"status":"ok","timestamp":1519748955893,"user_tz":300,"elapsed":2340,"user":{"displayName":"Louis Lv","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"101178496584126032730"}}},"cell_type":"code","source":["\n","#%%\n","# Vectorize the data.\n","\n","input_characters = set()\n","target_characters = set()\n","def loadData(corpuspath,size=None):\n","    input_texts = []\n","    target_texts = []\n","    with gzip.open(corpuspath, 'rt', encoding='ISO-8859-1') as f:\n","        lines = f.read().split('\\n')\n","    input_phrase=[]\n","    target_phrase=[]\n","    # for line in lines[: min(num_samples, len(lines) - 1)]:\n","    i=0\n","    for line in lines:\n","        if not line.startswith('#begin') and not line.startswith('#end') and len(line.split('\\t'))>1:\n","            line=line.encode(\"ascii\", errors=\"ignore\").decode()\n","            target_word, input_word = line.split('\\t')\n","            input_word=input_word.strip().lower()\n","            target_word=target_word.strip().lower()\n","            input_phrase.append(input_word)\n","            target_phrase.append(target_word)\n","            input_phrase.append(' ')\n","            target_phrase.append(' ')\n","            if input_word=='.':\n","                # We use \"tab\" as the \"start sequence\" character\n","                # for the targets, and \"\\n\" as \"end sequence\" character.\n","                input_texts.append(''.join(input_phrase))\n","                target_texts.append('\\t'+''.join(target_phrase) + '\\n')\n","                input_phrase=[]\n","                target_phrase=[]\n","                i+=1\n","\n","            for char in input_word:\n","                if char not in input_characters:\n","                    input_characters.add(char)\n","            for char in target_word:\n","                if char not in target_characters:\n","                    target_characters.add(char)\n","            if size is not None and i>size: break\n","            \n","    return input_texts,target_texts\n","    \n","input_texts,target_texts=loadData('data/train-1544.gz',5500)\n","size_train=len(input_texts)\n","test_input_texts,test_target_texts=loadData('data/test-2834.gz',2000)\n","size_test=len(test_input_texts)\n","\n","\n","input_texts=np.array(input_texts)\n","target_texts=np.array(target_texts)\n","# np.random.seed(456)\n","# indexs = np.arange(0,len(input_texts))\n","# np.random.shuffle(indexs)\n","# input_texts = input_texts[indexs[0:size_train]]\n","# target_texts=target_texts[indexs[0:size_train]]\n","\n","test_input_texts=np.array(test_input_texts)\n","test_target_texts=np.array(test_target_texts)\n","\n","#input_texts,target_texts: list of phrases\n","#input_token_index: key-values of char\n","#encoder_input_data:3dimension, d1=phrase,d2=char index in phrase,d3=index of list of candidate chars\n","# print(input_texts[0:2])\n","# print(target_texts[0:2])\n","\n","input_characters.add(' ')\n","target_characters.add(' ')\n","target_characters.add('\\t')\n","target_characters.add('\\n')\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","print('Number of sentence samples:', len(input_texts))\n","print('Number of unique input tokens:', num_encoder_tokens)\n","print('Number of unique output tokens:', num_decoder_tokens)\n","print('Max sequence length for inputs:', max_encoder_seq_length)\n","print('Max sequence length for outputs:', max_decoder_seq_length)\n","print('sentence of training data:', size_train)\n","print('sentence of test data:', size_test)\n","\n","input_token_index = dict(\n","    [(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict(\n","    [(char, i) for i, char in enumerate(target_characters)])\n","\n","# print(input_token_index)\n","# print(target_token_index)\n","\n","encoder_input_data = np.zeros(\n","    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n","    dtype='float32')\n","decoder_input_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","decoder_target_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Number of sentence samples: 5501\n","Number of unique input tokens: 60\n","Number of unique output tokens: 62\n","Max sequence length for inputs: 817\n","Max sequence length for outputs: 841\n","sentence of training data: 5501\n","sentence of test data: 2001\n"],"name":"stdout"}]},{"metadata":{"id":"DsdNIzRVWnjT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.\n","    for t, char in enumerate(target_text):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t, target_token_index[char]] = 1.\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n","\n","# Define an input sequence and process it.\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","encoder = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","# We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","# We set up our decoder to return full output sequences,\n","# and to return internal states as well. We don't use the\n","# return states in the training model, but we will use them in inference.\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n","                                     initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model that will turn\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tOuVhGSKWbMP","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":282}],"base_uri":"https://localhost:8080/","height":202},"outputId":"879b1be5-2706-4fae-c826-d612e5c9f086"},"cell_type":"code","source":["\n","#%%\n","# Run training\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          validation_split=0.2)\n","# Save model\n","# model.save('s2s.h5')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 4400 samples, validate on 1101 samples\n","Epoch 1/100\n","4400/4400 [==============================] - 194s 44ms/step - loss: 0.4160 - val_loss: 0.4815\n","Epoch 2/100\n","4400/4400 [==============================] - 194s 44ms/step - loss: 0.3825 - val_loss: 0.4397\n","Epoch 3/100\n","4400/4400 [==============================] - 195s 44ms/step - loss: 0.3476 - val_loss: 0.4045\n","Epoch 4/100\n","4400/4400 [==============================] - 194s 44ms/step - loss: 0.3247 - val_loss: 0.3864\n","Epoch 5/100\n","  64/4400 [..............................] - ETA: 2:56 - loss: 0.3496"],"name":"stdout"}]},{"metadata":{"id":"_67oPKMN_cG-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(os.getcwd())\n","from keras.models import load_model\n","model.save('model1-lstm-4016samples-100epochs.h5')\n","# del model  # deletes the existing model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tupt0rDIWizf","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","#%%\n","# Next: inference mode (sampling).\n","# Here's the drill:\n","# 1) encode input and retrieve initial decoder state\n","# 2) run one step of decoder with this initial state\n","# and a \"start of sequence\" token as target.\n","# Output will be the next target token\n","# 3) Repeat with the current target token and current states\n","\n","# Define sampling models\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states)\n","\n","# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","reverse_input_char_index = dict(\n","    (i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict(\n","    (i, char) for char, i in target_token_index.items())\n","\n","\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, target_token_index['\\t']] = 1.\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if (sampled_char == '\\n' or\n","           len(decoded_sentence) > max_decoder_seq_length):\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence\n","  \n","\n","#%%\n","'''\n","Functions of Evalutate the prediction\n","'''\n","def count_accuracy(pred_sents,target_sents):\n","    count_accu=0\n","    total=0\n","    for pred_sent,target_sent in zip(pred_sents,target_sents):\n","        pred_list=re.split(r\"-| |\\?\",pred_sent)\n","        # pred_list=pred_sent.split(' ')\n","        for pred_token,target_token in zip(pred_list,target_sent):\n","            total+=1\n","            if pred_token==target_token.text:\n","                count_accu+=1\n","    return count_accu, total\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"614k9nM4WcmT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":21}],"base_uri":"https://localhost:8080/","height":1398},"outputId":"1e040b02-5653-4b1e-8b5e-26b26ac84bc4","executionInfo":{"status":"ok","timestamp":1519414182171,"user_tz":300,"elapsed":10633,"user":{"displayName":"Louis Lv","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"101178496584126032730"}}},"cell_type":"code","source":["\n","#%%\n","print('-------predict train data:')\n","for seq_index in range(20):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_input_data[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["-------predict train data:\n","-\n","Input sentence:  he also have a brother who be a well-respected professor and glaciologist , and a sister ( now decease ) who be also a scientist and a professor . \n","Decoded sentence:  he was a september 1960 and 1998 , was deserviced as a second region and on the second red sox . \n","\n","-\n","Input sentence:  in 1977 environmental group present the petition to parliament carry 341,160 signature . \n","Decoded sentence:  in 1981 , the population of the post of the mountain to around 2011 in a season of the mountain to also known as the destroy records . \n","\n","-\n","Input sentence:  it be threaten by habitat loss . \n","Decoded sentence:  it is threatened by habitat loss . \n","\n","-\n","Input sentence:  abutilon eremitopetalum , commonly know as the hidden-petaled abutilon or hiddenpetal indian mallow , be a species of flowering shrub in the mallow family , malvaceae . \n","Decoded sentence:  a star the community consists of the south community of the united states , and the first community . \n","\n","-\n","Input sentence:  a major step toward the pennant have be take with the doyle deal . \n","Decoded sentence:  a star the community consists of the south community of the united states , and the first community . \n","\n","-\n","Input sentence:  vsn 's mission be to give fan , coach , and player of small college athletics a place to find out the latest information on the naia . \n","Decoded sentence:  during the season , the second red sox season in the mountain the first community , and was a settlement of the community . \n","\n","-\n","Input sentence:  consistent with its stated provision , the motorcycle safety foundation withdraw support of the abbreviated study , say such a study would be `` unlikely to either validate the finding of prior study or establish , to any statistical significant level , any new causative factor . \n","Decoded sentence:  on june 17 , 2007 , the campase a compress of the contral taken in 1967 and was a second warbler . \n","\n","-\n","Input sentence:  the boston red sox season be a season in american baseball . \n","Decoded sentence:  the company 's south korean second as a season and a community in the umerican league . \n","\n","-\n","Input sentence:  in lead the league in home run , rbi , and batting average , yastrzemski achieve the triple crown . \n","Decoded sentence:  in 1981 , the population of the post of the mountain to around 2011 in a season of the mountain to also known as the destroy records . \n","\n","-\n","Input sentence:  the ceylon durian , a large prickly fruit , be inedible and do not stink . \n","Decoded sentence:  the company 's south korean second as a season and a community in the umerican league . \n","\n","-\n","Input sentence:  then he be call out by the umpire immediately after for interfere with the play at first base . \n","Decoded sentence:  the second was a september 1968 , and was commissioned by the decision of the contral game . \n","\n","-\n","Input sentence:  out of anger the man kill she . \n","Decoded sentence:  cola ligental intend to the red sox finished the red sox finished the red sox finished the red sox finished the red sox finished the red sox . \n","\n","-\n","Input sentence:  the school also have a full service cafeteria that serve breakfast as well as lunch . \n","Decoded sentence:  the second warbler ( phylloscopus origines ) is a species of old world warbler in the locustellidae family . \n","\n","-\n","Input sentence:  it be threaten by habitat loss . \n","Decoded sentence:  it is threatened by habitat loss . \n","\n","-\n","Input sentence:  pentace perakensis be a species of flowering plant in the malvaceae sensu lato or tiliacea family . \n","Decoded sentence:  marchand was a separate summins and early and the destroy of the transical company . \n","\n","-\n","Input sentence:  the grey emutail ( dromaeocercus seebohmus ) , also know as the madagascan grassbird or feather-tailed warbler , be a species of old world warbler in the locustellidae family . \n","Decoded sentence:  the second warbler ( phylloscopus origines ) is a species of old world warbler in the locustellidae family . \n","\n","-\n","Input sentence:  clouscard 's graduate study in letter and philosophy under the tutelage of henri lefebvre culminate with a thesis , the be and the code ( publish 1972 ) , present to jean-paul sartre , among other , for defense . \n","Decoded sentence:  briggs was recorded at the community of the baseball completion of the community community . \n","\n","-\n","Input sentence:  it be approve by the fda in april 2011 . \n","Decoded sentence:  it is threatened by habitat loss . \n","\n","-\n","Input sentence:  in 2007 tony o'brien leave the band to be replace briefly by dave francis . \n","Decoded sentence:  in 1981 , the population of the post of the mountain to around 2011 in a season of the mountain to also known as the destroy records . \n","\n","-\n","Input sentence:  faulks also comment that `` some people find it perplexing but i think the way that the book have be present ... be a clever way of show that it be not my book , although , of course , it be my book '' . \n","Decoded sentence:  most lake started the bartast was a september 1960 and 1998 , when it became a season of the community . \n","\n"],"name":"stdout"}]},{"metadata":{"id":"tDAbeHLx9_Pt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(os.getcwd())\n","from keras.models import load_model\n","model.load('model1-lstm-4016samples-100epochs.h5')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IGqYT6IICIhz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","#%%\n","print('-------predict train data:')\n","train_pred_sents=[]\n","for seq_index in range(500):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_input_data[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-- No. ',seq_index)\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)\n","    train_pred_sents.append(decoded_sentence)\n","\n","train_acc_count,count_total=count_accuracy(train_pred_sents,input_texts)\n","print('Accuracy on LSTM1 predicteur, train data:', train_acc_count,'/', count_total,'=',train_acc_count/count_total)\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"G1YfE1qXEItM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","test_encoder_input_data = np.zeros(\n","    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n","    dtype='float32')\n","# test_decoder_input_data = np.zeros(\n","#     (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","#     dtype='float32')\n","# test_decoder_target_data = np.zeros(\n","#     (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","\n","for i, (test_input_text, test_target_text) in enumerate(zip(test_input_texts, test_target_texts)):\n","    for t, char in enumerate(test_input_text):\n","        test_encoder_input_data[i, t, input_token_index[char]] = 1.\n","#     for t, char in enumerate(test_target_text):\n","#         # decoder_target_data is ahead of decoder_input_data by one timestep\n","#         test_decoder_input_data[i, t, target_token_index[char]] = 1.\n","#         if t > 0:\n","#             # decoder_target_data will be ahead by one timestep\n","#             # and will not include the start character.\n","#             test_decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n","\n","\n","print('-----predict test data:')\n","test_pred_sents=[]\n","for seq_index in range(500):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = test_encoder_input_data[seq_index]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-- No. ',seq_index)\n","    print('Input test sentence:', test_input_texts[seq_index])\n","    print('Decoded test sentence:', decoded_sentence)\n","    test_pred_sents.append(decoded_sentence)\n","\n","test_acc_count,count_total=count_accuracy(test_pred_sents,test_input_texts)\n","print('Accuracy on LSTM1 predicteur, test data:', test_acc_count,'/', count_total,'=',test_acc_count/count_total)\n","\n","\n","\n"],"execution_count":0,"outputs":[]}]}