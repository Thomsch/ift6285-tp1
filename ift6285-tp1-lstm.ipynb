{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ift6285-tp1-lstm.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"ycul1k1wW-_y","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"15f8f412-21cd-4831-d61a-bb7385dc6da2","executionInfo":{"status":"ok","timestamp":1519747013359,"user_tz":300,"elapsed":9615,"user":{"displayName":"Louis Lv","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"101178496584126032730"}}},"cell_type":"code","source":["# https://keras.io/\n","!pip install -q keras\n","import keras"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"H-IVQBM9gRXB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":6}],"base_uri":"https://localhost:8080/","height":138},"outputId":"3dfa8da7-0941-4e98-e5bc-c3e648442df5","executionInfo":{"status":"ok","timestamp":1519747074833,"user_tz":300,"elapsed":12951,"user":{"displayName":"Louis Lv","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"101178496584126032730"}}},"cell_type":"code","source":["# Install a Drive FUSE wrapper.\n","# https://github.com/astrada/google-drive-ocamlfuse\n","!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","\n","# Generate auth tokens for Colab\n","from google.colab import auth\n","auth.authenticate_user()\n","\n","# Generate creds for the Drive FUSE library.\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n","\n","!mkdir -p drive\n","!google-drive-ocamlfuse drive\n","\n","print('Files in Drive:')\n","!ls /content/drive/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n","Files in Drive:\n","Colab Notebooks  created.txt  data  Louis共享文件夹  M.Sc-DIRO-UdeM  test.ods\n"],"name":"stdout"}]},{"metadata":{"id":"0ezTCTnhWOlS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1},{"item_id":2}],"base_uri":"https://localhost:8080/","height":51},"outputId":"f5fc1cdd-ebc2-427b-967e-8f9e8dc6f416","executionInfo":{"status":"ok","timestamp":1519763291231,"user_tz":300,"elapsed":1056,"user":{"displayName":"Louis Lv","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"101178496584126032730"}}},"cell_type":"code","source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\n","#%%\n","from __future__ import print_function\n","\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","import numpy as np\n","import gzip\n","\n","\n","batch_size = 128  # Batch size for training.\n","epochs = 100  # Number of epochs to train for.\n","latent_dim = 256  # Latent dimensionality of the encoding space.\n","num_samples = 10000  # Number of samples to train on.\n","# Path to the data txt file on disk.\n","\n","import os\n","os.chdir(\"/content/drive/M.Sc-DIRO-UdeM/IFT6285-Traitements automatique des langues naturelles/TP1/ift6285-tp1\")\n","# os.chdir(\"/Users/louis/Google Drive/M.Sc-DIRO-UdeM/IFT6285-Traitements automatique des langues naturelles/TP1\")\n","# os.chdir(\"/Users/fanxiao/Google Drive/UdeM/IFT6135 Representation Learning/homework1/programming part \")\n","print(os.getcwd())\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/M.Sc-DIRO-UdeM/IFT6285-Traitements automatique des langues naturelles/TP1/ift6285-tp1\n"],"name":"stdout"}]},{"metadata":{"id":"C5nsykhhWXcb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":135},"outputId":"ead0b765-184c-49e5-8ee7-f25e5df915d1","executionInfo":{"status":"ok","timestamp":1519763292864,"user_tz":300,"elapsed":969,"user":{"displayName":"Louis Lv","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"101178496584126032730"}}},"cell_type":"code","source":["\n","#%%\n","# Vectorize the data.\n","\n","input_characters = set()\n","target_characters = set()\n","def loadData(corpuspath,size=None):\n","    input_texts = []\n","    target_texts = []\n","    with gzip.open(corpuspath, 'rt', encoding='ISO-8859-1') as f:\n","        lines = f.read().split('\\n')\n","    input_phrase=[]\n","    target_phrase=[]\n","    # for line in lines[: min(num_samples, len(lines) - 1)]:\n","    i=0\n","    for line in lines:\n","        if not line.startswith('#begin') and not line.startswith('#end') and len(line.split('\\t'))>1:\n","            line=line.encode(\"ascii\", errors=\"ignore\").decode()\n","            target_word, input_word = line.split('\\t')\n","            input_word=input_word.strip().lower()\n","            target_word=target_word.strip().lower()\n","            input_phrase.append(input_word)\n","            target_phrase.append(target_word)\n","            input_phrase.append(' ')\n","            target_phrase.append(' ')\n","            if input_word=='.':\n","                # We use \"tab\" as the \"start sequence\" character\n","                # for the targets, and \"\\n\" as \"end sequence\" character.\n","                input_texts.append(''.join(input_phrase))\n","                target_texts.append('\\t'+''.join(target_phrase) + '\\n')\n","                input_phrase=[]\n","                target_phrase=[]\n","                i+=1\n","\n","            for char in input_word:\n","                if char not in input_characters:\n","                    input_characters.add(char)\n","            for char in target_word:\n","                if char not in target_characters:\n","                    target_characters.add(char)\n","            if size is not None and i>size: break\n","            \n","    return input_texts,target_texts\n","    \n","input_texts,target_texts=loadData('data/train-1544.gz',5000)\n","size_train=len(input_texts)\n","test_input_texts,test_target_texts=loadData('data/test-2834.gz',1000)\n","size_test=len(test_input_texts)\n","\n","\n","input_texts=np.array(input_texts)\n","target_texts=np.array(target_texts)\n","# np.random.seed(456)\n","# indexs = np.arange(0,len(input_texts))\n","# np.random.shuffle(indexs)\n","# input_texts = input_texts[indexs[0:size_train]]\n","# target_texts=target_texts[indexs[0:size_train]]\n","\n","test_input_texts=np.array(test_input_texts)\n","test_target_texts=np.array(test_target_texts)\n","\n","#input_texts,target_texts: list of phrases\n","#input_token_index: key-values of char\n","#encoder_input_data:3dimension, d1=phrase,d2=char index in phrase,d3=index of list of candidate chars\n","# print(input_texts[0:2])\n","# print(target_texts[0:2])\n","\n","input_characters.add(' ')\n","target_characters.add(' ')\n","target_characters.add('\\t')\n","target_characters.add('\\n')\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","print('Number of sentence samples:', len(input_texts))\n","print('Number of unique input tokens:', num_encoder_tokens)\n","print('Number of unique output tokens:', num_decoder_tokens)\n","print('Max sequence length for inputs:', max_encoder_seq_length)\n","print('Max sequence length for outputs:', max_decoder_seq_length)\n","print('sentence of training data:', size_train)\n","print('sentence of test data:', size_test)\n","\n","input_token_index = dict(\n","    [(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict(\n","    [(char, i) for i, char in enumerate(target_characters)])\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Number of sentence samples: 5001\n","Number of unique input tokens: 60\n","Number of unique output tokens: 62\n","Max sequence length for inputs: 817\n","Max sequence length for outputs: 841\n","sentence of training data: 5001\n","sentence of test data: 1001\n"],"name":"stdout"}]},{"metadata":{"id":"DsdNIzRVWnjT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","# print(input_token_index)\n","# print(target_token_index)\n","\n","encoder_input_data = np.zeros(\n","    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n","    dtype='float32')\n","decoder_input_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","decoder_target_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","\n","for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.\n","    for t, char in enumerate(target_text):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t, target_token_index[char]] = 1.\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n","\n","# Define an input sequence and process it.\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","encoder = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","# We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","# We set up our decoder to return full output sequences,\n","# and to return internal states as well. We don't use the\n","# return states in the training model, but we will use them in inference.\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n","                                     initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model that will turn\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tOuVhGSKWbMP","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":40}],"base_uri":"https://localhost:8080/","height":101},"outputId":"8b55a81a-7fc6-447e-a3cd-bb3fa6c7edfc"},"cell_type":"code","source":["\n","#%%\n","# Run training\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          validation_split=0.2)\n","# Save model\n","# model.save('s2s.h5')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 4000 samples, validate on 1001 samples\n","Epoch 1/100\n","4000/4000 [==============================] - 91s 23ms/step - loss: 0.4236 - val_loss: 0.4757\n","Epoch 2/100\n"," 768/4000 [====>.........................] - ETA: 1:07 - loss: 0.3961"],"name":"stdout"}]},{"metadata":{"id":"_67oPKMN_cG-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(os.getcwd())\n","from keras.models import load_model\n","model.save('model1-lstm-4016samples-100epochs.h5')\n","# del model  # deletes the existing model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tupt0rDIWizf","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","#%%\n","# Next: inference mode (sampling).\n","# Here's the drill:\n","# 1) encode input and retrieve initial decoder state\n","# 2) run one step of decoder with this initial state\n","# and a \"start of sequence\" token as target.\n","# Output will be the next target token\n","# 3) Repeat with the current target token and current states\n","\n","# Define sampling models\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states)\n","\n","# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","reverse_input_char_index = dict(\n","    (i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict(\n","    (i, char) for char, i in target_token_index.items())\n","\n","\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, target_token_index['\\t']] = 1.\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if (sampled_char == '\\n' or\n","           len(decoded_sentence) > max_decoder_seq_length):\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence\n","  \n","\n","#%%\n","'''\n","Functions of Evalutate the prediction\n","'''\n","def count_accuracy(pred_sents,target_sents):\n","    count_accu=0\n","    total=0\n","    for pred_sent,target_sent in zip(pred_sents,target_sents):\n","        pred_list=re.split(r\"-| |\\?\",pred_sent)\n","        # pred_list=pred_sent.split(' ')\n","        for pred_token,target_token in zip(pred_list,target_sent):\n","            total+=1\n","            if pred_token==target_token.text:\n","                count_accu+=1\n","    return count_accu, total\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"614k9nM4WcmT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":3},{"item_id":4}],"base_uri":"https://localhost:8080/","height":1426},"outputId":"b921258d-0926-4d39-d65d-5332b70bd20c","executionInfo":{"status":"error","timestamp":1519763077001,"user_tz":300,"elapsed":5632,"user":{"displayName":"Louis Lv","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"101178496584126032730"}}},"cell_type":"code","source":["\n","#%%\n","print('-------predict train data:')\n","for seq_index in range(20):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_input_data[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["-------predict train data:\n","-\n","Input sentence: the rimatara reed warbler ( acrocephalus rimitara ) be a species of old world warbler in the acrocephalidae family . \n","Decoded sentence: /\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\n","-\n","Input sentence:  it be find only in french polynesium . \n","Decoded sentence: /\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-5cb500167eb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# for trying out decoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input sentence:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-eb486a73a769>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         output_tokens, h, c = decoder_model.predict(\n\u001b[0;32m---> 37\u001b[0;31m             [target_seq] + states_value)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Sample a token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1842\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m     def train_on_batch(self, x, y,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1335\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"tDAbeHLx9_Pt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"e92eed2b-7e71-4cd3-f51b-1368e1258803","executionInfo":{"status":"ok","timestamp":1519763089445,"user_tz":300,"elapsed":4196,"user":{"displayName":"Louis Lv","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"101178496584126032730"}}},"cell_type":"code","source":["print(os.getcwd())\n","from keras.models import load_model\n","model=load_model('output-lstm/model1-lstm-4016samples-100epochs.h5')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["/content/drive/M.Sc-DIRO-UdeM/IFT6285-Traitements automatique des langues naturelles/TP1/ift6285-tp1\n"],"name":"stdout"}]},{"metadata":{"id":"IGqYT6IICIhz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":4},{"item_id":5}],"base_uri":"https://localhost:8080/","height":771},"outputId":"5e39af67-ba32-49f5-ae63-8f4f517d8b54","executionInfo":{"status":"error","timestamp":1519763101292,"user_tz":300,"elapsed":9699,"user":{"displayName":"Louis Lv","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"101178496584126032730"}}},"cell_type":"code","source":["\n","#%%\n","print('-------predict train data:')\n","train_pred_sents=[]\n","for seq_index in range(500):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_input_data[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-- No. ',seq_index)\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)\n","    train_pred_sents.append(decoded_sentence)\n","\n","train_acc_count,count_total=count_accuracy(train_pred_sents,input_texts)\n","print('Accuracy on LSTM1 predicteur, train data:', train_acc_count,'/', count_total,'=',train_acc_count/count_total)\n","\n","\n","\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["-------predict train data:\n","-- No.  0\n","Input sentence: the rimatara reed warbler ( acrocephalus rimitara ) be a species of old world warbler in the acrocephalidae family . \n","Decoded sentence: /\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\n","-- No.  1\n","Input sentence:  it be find only in french polynesium . \n","Decoded sentence: /\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\n","-- No.  2\n","Input sentence:  its natural habitat be subtropical or tropical dry forest and swamp . \n","Decoded sentence: /\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-ebc3bd20120d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# for trying out decoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-- No. '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input sentence:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-eb486a73a769>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         output_tokens, h, c = decoder_model.predict(\n\u001b[0;32m---> 37\u001b[0;31m             [target_seq] + states_value)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Sample a token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1842\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m     def train_on_batch(self, x, y,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0mindices_for_conversion_to_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"G1YfE1qXEItM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","test_encoder_input_data = np.zeros(\n","    (len(test_input_texts), max_encoder_seq_length, num_encoder_tokens),\n","    dtype='float32')\n","test_decoder_input_data = np.zeros(\n","    (len(test_input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","test_decoder_target_data = np.zeros(\n","    (len(test_input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","\n","\n","for i, (test_input_text, test_target_text) in enumerate(zip(test_input_texts, test_target_texts)):\n","    for t, char in enumerate(test_input_text):\n","        test_encoder_input_data[i, t, input_token_index[char]] = 1.\n","    for t, char in enumerate(test_target_text):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        test_decoder_input_data[i, t, target_token_index[char]] = 1.\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            test_decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"V5g3kL_34uOo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":5},{"item_id":6}],"base_uri":"https://localhost:8080/","height":1527},"outputId":"908a43be-45a6-4410-d7d6-31fec66175d6","executionInfo":{"status":"error","timestamp":1519763180187,"user_tz":300,"elapsed":12265,"user":{"displayName":"Louis Lv","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"101178496584126032730"}}},"cell_type":"code","source":["\n","\n","print('-----predict test data:')\n","test_pred_sents=[]\n","for seq_index in range(500):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-- No. ',seq_index)\n","    print('Input test sentence:', test_input_texts[seq_index])\n","    print('Decoded test sentence:', decoded_sentence)\n","    test_pred_sents.append(decoded_sentence)\n","\n","test_acc_count,count_total=count_accuracy(test_pred_sents,test_input_texts)\n","print('Accuracy on LSTM1 predicteur, test data:', test_acc_count,'/', count_total,'=',test_acc_count/count_total)\n","\n","\n","\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["-----predict test data:\n","-- No.  0\n","Input test sentence: shorty 's lunch be a washington , pennsylvania-based hot dog lunch counter . \n","Decoded test sentence: /\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\n","-- No.  1\n","Input test sentence:  a `` local landmark , '' shorty 's have be own by the alexas family since the 1930 . \n","Decoded test sentence: /\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\n","-- No.  2\n","Input test sentence:  it have two location , include the main facility on west chestnut street in washington , as well as in canton . \n","Decoded test sentence: /\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\n","-- No.  3\n","Input test sentence:  the main restaurant boast old wooden booth , a dining counter , as well a large storefront , show the grill to pedestrian . \n","Decoded test sentence: /\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\t/\t\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-07ce8f9beaab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# for trying out decoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_encoder_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-- No. '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input test sentence:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-eb486a73a769>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         output_tokens, h, c = decoder_model.predict(\n\u001b[0;32m---> 37\u001b[0;31m             [target_seq] + states_value)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Sample a token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1842\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m     def train_on_batch(self, x, y,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1335\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"-skHmaFf6GFe","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}